\chapter{Experiments}
\label{chapter05}

\section{Comparing AIs against each other}

To compare the strength of our AIs against each other, we simulated 1000 randomly generated games
and had the AIs play against each other. Both teams were generated at random and there was no step taken
to make them balanced. Instead, each AI played both sides of the match. See \autoref{tab:winrates} for results.

\begin{table}[h]
	\centering
	\begin{tabular}{|| c | c c c ||}
		\hline
		& MCTS & Rule & Random \\
		\hline\hline
		MCTS & N/A & 63\% & 88\% \\
		Rule & 37\%& N/A & 82\% \\
		Random & 12\% & 18\% & N/A \\
		\hline
	\end{tabular}
	\caption{Table showing win percentages over 1000 games between our different AI implementations.
	MCTS beats Random 88\% of the time, MCTS beats Rule 63\% of the time, and Rule beats Random 82\% of the time.}
	\label{tab:winrates}
\end{table}

As we can see, both MCTS and the Rule Based AI can beat our Random AI with a significant margin,
and MCTS can also consistently beat the Rule Based AI, despite playing for both generated teams.
From this we conclude that:

\begin{enumerate}
	\item A greedy aggressive strategy doesn't always win, which means the game mechanics are rich enough
	to reward strategic thinking.
	\item Thinking ahead (as MCTS does) provides enough value to be significant in terms of win rate.
\end{enumerate}

\section{Survey}
\label{survey}

We've conducted a small survey to verify the results of our encounter balancing algorithm.
The participants played 126 games in total and filled in a questionnaire after each game. Half of the encounters
in the games were completely generated by our algorithm, and in the other half we designed one team and let
the algorithm generate a suitable opponent.

\subsection{Participants}

The participants were all computer science students. All of them had at least
some experience with computer games, and were presented the mechanics of HexMage
before conducting the experiment.

\subsection{Experiment Design}

The goal of the survey is to measure two things. One, if the generated
encounters are actually balanced. And two, if the AI is strong enough to pose a
challenge to the player. We will measure the overall winrate of the players,
and also compare how the AI did in games the player considered balanced.

The experiment consists of 20 different games of HexMage. All of the games are
played on the same map that was hand-designed beforehand. This was to allow the
participants to better get familiar with the game and think ahead. The games
are structured so that each player has 2 mages, each with 2 abilities. This was
to reduce the cognitive overhead for the participants and allow them to more easily
adjust to the 20 completely different scenarios.

The first 10 of the 20 games had the player team hand designed, and the
opponent (played by the AI) generated with our PCG algorithm. The remaining 10
games had both teams generated with no manual tweaks or changes. Having some of
the games hand designed allows us to show that the PCG algorithm can balance
against constraints that aren't completely random. A hand designed team might
have features that are rare in the search space. All of the games are played
against the MCTS based AI\@ with a fixed number of iterations.

\subsection{Questions}

The questions were the following, answered on scale (1 - definitely no, to 7 - definitely yes), showing short codes for \autoref{tab:balance-corr} and \autoref{tab:difficulty-corr}

\begin{description}[]
	\item[Balanced:] The game balanced.
	\item[Challenge:] The game was challenging.
	\item[Unsure:] I wasn't sure who was going to win.
	\item[Smart:] The AI played smart.
	\item[Difficult:] The game was difficult.
	\item[Strategy:] The AI showed strategic thinking.
\end{description}

\subsection{Discussion of the Results}

As we can see in \autoref{tab:balance-corr}, the questions for balance strongly correlate.

\begin{table}[h]
	\centering
	\begin{tabular}{lrr}
		\toprule
		{} &  balanced &  unsure \\
		\midrule
		balanced &      1.00 &    \cellcolor{blue!25}0.73 \\
		unsure   &      0.73 &    1.00 \\
		\bottomrule
	\end{tabular}
	\caption{Correlation table between people who said the game was balanced and who were unsure about the result.}	
	\label{tab:balance-corr}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{lrrrr}
		\toprule
		{} &  challenge &  smart &  difficult &  strategy \\
		\midrule
		challenge &       1.00 &   0.61 &       0.46 &      0.63 \\
		smart     &       0.61 &   1.00 &       0.39 &      \cellcolor{blue!25}0.82 \\
		difficult &       0.46 &   0.39 &       1.00 &      0.39 \\
		strategy  &       0.63 &   0.82 &       0.39 &      1.00 \\
		\bottomrule
	\end{tabular}
	\caption{Correlation table between people who said the game was challenging, the AI was smart, the game was difficult to play and the AI showed strategic behavior.}
	\label{tab:difficulty-corr}
\end{table}


We can also take a look at the responses normalized to $0$ and $1$ where $1$ means
the response was at least $4$. Looking at \autoref{tab:norm-corr}, we can see that players
tend to consider the game difficult if they lost and vice versa.

\begin{table}[h]
	\centering
	\begin{tabular}{lrrrrrrr}
		\toprule
		{} &  balanced &  challenge &  unsure &  smart &  difficult &  strategy &   won \\
		\midrule
		balanced  &      1.00 &       0.35 &    0.53 &   0.21 &       0.07 &      0.22 &  0.15 \\
		challenge &      0.35 &       1.00 &    0.30 &   0.42 &       0.48 &      0.36 & -0.20 \\
		unsure    &      0.53 &       0.30 &    1.00 &   0.17 &       0.16 &      0.14 &  0.07 \\
		smart     &      0.21 &       0.42 &    0.17 &   1.00 &       0.49 &      0.79 & -0.32 \\
		difficult &      0.07 &       0.48 &    0.16 &   0.49 &       1.00 &      0.42 & \cellcolor{blue!25}-0.62 \\
		strategy  &      0.22 &       0.36 &    0.14 &   0.79 &       0.42 &      1.00 & -0.22 \\
		won       &      0.15 &      -0.20 &    0.07 &  -0.32 &      -0.62 &     -0.22 &  1.00 \\
		\bottomrule
	\end{tabular}
	\caption{Table of correlations after normalizing reponse values from 1--7 to 0--1 where 1 means the original response was at least 0.}
	\label{tab:norm-corr}
\end{table}

The total win rate of the players was only $38\%$, which shows that our MCTS~AI
is a formidable opponent. \autoref{tab:means} show the mean of the other responses.

\begin{table}[h!]
	\centering
	\begin{tabular}{lr}
		\toprule
		{} & mean \\ 
		\midrule
		balanced        &       0.60\% \\
		challenge       &       0.71\% \\
		unsure  &       0.57\% \\
		smart   &       0.78\% \\
		difficult       &       0.76\% \\
		strategy        &       0.78\% \\
		\bottomrule
	\end{tabular}
	\caption{Table of mean responses to all the questions in the questionnaire.}
	\label{tab:means}
\end{table}

Looking further at the results, in $4$ out of the $20$ games the players lost $100\%$ of the time,
and in $1$ out of the $20$ games the players won $100\%$ of the time. In the resulting $15$ games
there were both cases when a player won and when a player lost. We re-evaluated those games manually 
and found that in one case the game trully could not have been won, as one of the player mages always
got killed too early on and did not get to do any damage. On the other hand, the second game we tried
we won rather decisively, even though all of the participants in the experiment lost. The strategy to winning
strategy was however to play very defensively and calculate the opponents AP,
which is something the participants probably did not have a chance to do.

As for the third game, we also confirm it as impossible to win, as the opponent had a cheap ability and powerful ability
and could easily stay out of range while doing damage. The last of the always-lost games was also similar
in the sense that the opponent had a cheap ability he could use over and over again, and once he got a small
advantage he could push through to win the game without any resistance. While the games were close in terms
of HP at the end, there was no clear way to win.

In light of these results, there is a possibility for future work by incorporating an additional fitness
metric that makes sure the player can win the game, and not just that the games end up being close. However,
considering the players voted on $60\%$ of the games as balanced, we have definitely met our goal of
generating balanced encounters.

Note that all of the 20 games were generated without any human intervention, and the only input from the user
was to set the constants for attributes' range. The whole process of generating a balanced encounter is completely
automatic and replicable.
